{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_get_latent_space.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwsO+7jJixmOZ91jyhraHZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SylvainVerdy/NLP_playstore/blob/master/BERT_get_latent_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okrMB8bRBQz_",
        "colab_type": "code",
        "outputId": "9628b790-f0fb-4e60-8714-4332762d7336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!pip install transformers\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKN3Hn2kvHJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4-Snawutbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(query: str, choices: List[str]):\n",
        "    \"\"\"\n",
        "\n",
        "    :param query:\n",
        "    :param choices:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inputs = [tokenizer.encode_plus(query, choice, add_special_tokens=True)\n",
        "              for choice in choices]\n",
        "\n",
        "    max_len = min(max(len(t['input_ids']) for t in inputs), max_seq_len)\n",
        "    input_ids = [t['input_ids'][:max_len] +\n",
        "                  [0] * (max_len - len(t['input_ids'][:max_len])) for t in inputs]\n",
        "    attention_mask = [[1] * len(t['input_ids'][:max_len]) +\n",
        "                      [0] * (max_len - len(t['input_ids'][:max_len])) for t in inputs]\n",
        "    token_type_ids = [t['token_type_ids'][:max_len] +\n",
        "                  [0] * (max_len - len(t['token_type_ids'][:max_len])) for t in inputs]\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    token_type_ids = torch.tensor(token_type_ids)\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKhCgS21BEwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "max_seq_len = 64\n",
        "\n",
        "def hidden_states(choices, query):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)  # Batch size 1\n",
        "  input_ids, attention_mask, token_type_ids = encode(query, choices)\n",
        "\n",
        "  print(input_ids.shape)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "  last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "  return last_hidden_states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HyqmznaYA5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "78f2db38-44dd-47c0-9326-fbf7c70dd700"
      },
      "source": [
        "sentences = [\"test\",\"word\", \"what is a gaussian model\", \"fall\", \"embeddings\", \"travel on my best way to improve my data\"]\n",
        "query = \"what is a gaussian model?\"\n",
        "outputs = hidden_states(choices=sentences, query=query)\n",
        "print(outputs.shape)\n"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 20])\n",
            "torch.Size([6, 20, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTbkWEoW0U1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mytensor = torch.zeros([20, 6])\n",
        "np_outputs = outputs.detach().numpy()\n",
        "vect_mean = np.mean(np_outputs, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAlia-kCsQWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f618090a-6b65-4aba-bb63-cd8746151eaf"
      },
      "source": [
        "print(outputs.shape)\n",
        "X = vect_mean\n",
        "print(X.shape)\n",
        "y = torch.Tensor([1,2,3,4,2,5])\n",
        "print(y.shape)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 20, 768])\n",
            "(6, 768)\n",
            "torch.Size([6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuwV5nQkwZlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCj14W0pxweP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3f1a69a5-1523-4220-84df-98a110463827"
      },
      "source": [
        "model = xgboost.sklearn.XGBClassifier()\n",
        "model.fit(X,y)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqc9KSWNyBb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91c545b3-9478-400e-a9f3-07f90e760f8c"
      },
      "source": [
        "output = hidden_states(choices=[\"fall\"], query=query)\n",
        "np_output = output.detach().numpy()\n",
        "np_output = np.mean(np_output, axis=1)\n",
        "y_pred = model.predict(np_output)\n",
        "print(y_pred)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 12])\n",
            "[3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UggYZglRN6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}